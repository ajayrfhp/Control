{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "np.random.seed(17)\n",
    "random.seed(17)\n",
    "torch.manual_seed(17)\n",
    "import wget\n",
    "import os\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from fpl import FPL\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from player import Player\n",
    "from team import Team\n",
    "from data_processor import get_fpl, get_current_squad, get_teams, get_players, get_training_datasets\n",
    "from agent import Agent\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why pytorch lightning \n",
    "- Better engineering for deep learning\n",
    "    - Organises code, removes boiler plate, reduces errors, allows reproducibility. \n",
    "\n",
    "## Blocks of pytorch lightning\n",
    "- Model\n",
    "- Optimizer\n",
    "- Data \n",
    "- Training loop\n",
    "- Validation loop\n",
    "\n",
    "## Notes\n",
    "- Lightning automatically identifies and uses available GPUs. cuda() conversions are not needed. \n",
    "- Trainer object allows us to fit\n",
    "    - Trainer.fit, default max epochs is 1000\n",
    "    - To adopt lightning to GPU, pass gpus argument as 1 inside trainer. \n",
    "    - num_nodes argument provides you easy distributed training across 32 nodes. \n",
    "    - provdes 16 bit truncation. \n",
    "- Lightning provides a metrics module that allows you to measure accuracy etc easily. \n",
    "- Lightning takes care of doing backward poss, clearing optimizers. \n",
    "- Lightning_logs/ tracks hyper parameter, losses and models. \n",
    "    - Checkpointing is automatically setup. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajrfhp/anaconda3/envs/control/lib/python3.8/site-packages/pandas/core/frame.py:5171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n",
      "/home/ajrfhp/Control/Control_FPL/data_processor.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_player_features[\"total_points\"] = all_player_features[\"total_points\"].clip(0, max_player_points)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fefb7b7d0a0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7ff0b076c940>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opponent_feature_names = [\"npxG\",\"npxGA\"]\n",
    "player_feature_names = [\"total_points\", \"ict_index\", \"clean_sheets\", \"saves\", \"assists\"]\n",
    "window = 4\n",
    "teams = get_teams(team_feature_names=opponent_feature_names, visualize=False, window=window)\n",
    "players = await get_players(player_feature_names, opponent_feature_names, visualize=False, num_players=680, window=window)\n",
    "train_loader, test_loader, _ = get_training_datasets(players, teams)\n",
    "train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, window_size=4, num_features=7):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.num_features = num_features\n",
    "        self.dim = window_size*num_features\n",
    "        self.fc1 = nn.Linear(self.dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0], self.dim))\n",
    "        return self.fc1(x).reshape((-1, ))\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, window_size=4, num_features=7):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.fc1 = nn.RNN(num_features, num_features, num_layers=5)\n",
    "        self.fc2 = nn.Linear(num_features, 1)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(dims=(0, 2, 1))\n",
    "        i, _ = self.fc1(x)\n",
    "        o = self.fc2(i[:,-1, :])\n",
    "        return o\n",
    "\n",
    "\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, window_size=4, num_features=7, use_opponent_features=True, len_opponent_features=2, model_type='linear'):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.dim = window_size * num_features\n",
    "        self.use_opponent_features = use_opponent_features\n",
    "        self.len_opponent_features = len_opponent_features\n",
    "        if model_type == 'linear':\n",
    "            self.model = LinearModel(window_size, num_features)\n",
    "        else:\n",
    "            self.model = RNNModel(window_size, num_features)\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0]\n",
    "        inputs = x[:,:,:self.window_size]\n",
    "        outputs = x[:,0,self.window_size]\n",
    "        predictions = self.model.forward(inputs)\n",
    "        loss = nn.MSELoss()(predictions, outputs)\n",
    "        self.log(f'{self.model_type} = train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.training_step(batch, batch_idx)\n",
    "        self.log(f'{self.model_type} = val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "model = RNNModel()\n",
    "t = torch.zeros((5,7,4))\n",
    "model.forward(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | LinearModel | 29    \n",
      "--------------------------------------\n",
      "29        Trainable params\n",
      "0         Non-trainable params\n",
      "29        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajrfhp/anaconda3/envs/control/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:322: UserWarning: The number of training samples (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 62/62 [00:00<00:00, 70.74it/s, loss=0.794, v_num=43]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | RNNModel | 568   \n",
      "-----------------------------------\n",
      "568       Trainable params\n",
      "0         Non-trainable params\n",
      "568       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajrfhp/anaconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([500])) that is different to the input size (torch.Size([500, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  79%|███████▉  | 49/62 [00:07<00:01,  6.98it/s, loss=0.997, v_num=44]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajrfhp/anaconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([181])) that is different to the input size (torch.Size([181, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/62 [00:00<00:00, 1057.83it/s, loss=0.997, v_num=44] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajrfhp/anaconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([46])) that is different to the input size (torch.Size([46, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 62/62 [00:07<00:00,  8.50it/s, loss=0.997, v_num=44]\n"
     ]
    }
   ],
   "source": [
    "for model_type in ['linear','rnn']:\n",
    "    model = LightningModel(model_type=model_type, use_opponent_features=False, len_opponent_features=True)\n",
    "    condition = f'{model_type} = val_loss'\n",
    "    trainer = pl.Trainer(max_epochs=50)\n",
    "    trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03ca37dd8f16b9970374fbeef9cf49c08034cd4bc6a7199a4f4689b44b4426cc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('control': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
